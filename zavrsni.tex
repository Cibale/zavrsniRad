\documentclass[times, utf8, zavrsni]{fer}
\usepackage{booktabs}
\usepackage{listingsutf8}
\usepackage[titletoc]{appendix}
\usepackage[page]{appendix}
\renewcommand{\figurename}{Slika}
\renewcommand{\contentsname}{Sadržaj}
\renewcommand{\appendixname}{Dodatak}
\renewcommand\tablename{Tablica}
\renewcommand\bibname{Literatura}
\renewcommand\bibtoc{Literatura}
\renewcommand{\lstlistingname}{Primjer}
\captionsetup[lstlisting]{position=bottom}
\lstset{language=Java,
 basicstyle=\tiny\tt,
 showspaces=false,
 showstringspaces=false,
 breaklines=true
 }
\lstset{
    literate=%
    {ć}{{\'c}}1
    {č}{{\v{c}}}1
    {đ}{{\dj{}}}1
    {š}{{\v{s}}}1
    {ž}{{\v{z}}}1
    {Ć}{{\'C}}1
    {Č}{{\v{C}}}1
    {Đ}{{\DJ{}}}1
    {Š}{{\v{S}}}1
    {Ž}{{\v{Z}}}1
}
\begin{document}

% TODO: Navedite broj rada.
\thesisnumber{1337}

% TODO: Navedite naslov rada.
\title{Obrada podataka tehnologijom Apache Spark}

% TODO: Navedite vaše ime i prezime.
\author{Martin Matak}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik{Na ovoj stranici se nalazi izvornik.}

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{Zahvala - TODO \ldots :)}

\tableofcontents

\chapter{Uvod}
\section{Motivacija}
%Literatura: http://mob.hr/samsung-galaxy-s4-skrivene-mogucnosti-senzori-i-octa-5-benchmark/
Pametni mobilni uređaji postaju neizostavan dodatak svakog modernog čovjeka.\\
Većina pametnih mobilnih uređaja u sebi sadrži sljedeće senzore:
\begin{description}
	\item[akcelerometar] - elektromehanička komponenta koja mjeri sile ubrzanja;
	\item[barometar] - mehanički senzor za mjerenje atmosferskog pritiska (na trenutnoj lokaciji uređaja);
	\item[senzor svjetlosti] - mjeri intenzitet, tj. jačinu svjetlosti i uglavnom se nalazi s prednje strane uređaja, iznad ekrana;
	\item[senzor blizine] - u stanju prepoznati situacije kada mu neki objekt stoji u blizini - ovo omogućava automatske pozive prilikom primicanja telefona licu uz zaključavanje telefona da bi onemogućili slučajno prekidanje poziva uhom ili slično;
	\item[senzor gestikulacije] - prepoznaje kretnje ruke tako što detektira infracrvene zrake koje se reflektiraju - omogućuje nam djelomično upravljanje telefonom bez doticanja ekrana;
	\item[žiroskop] - uređaj koji se koristi za navigaciju i merenje kutne brzine;
	\item[geomagnetski senzor] - mjeri okolno geomagnetsko polje za sve tri fizičke osi i u biti služi kao kompas na mobilnim uređajima i 
	\item[\emph{Hall Sensor}] - magnetski senzor zadužen za prepoznavanje je li  maska telefona zatvorena ili otvorena.
\end{description}
Pretpostavimo da je mobilni uređaj spojen na internet i da svakih nekoliko sekundi pošalje vrijednost koju u tom trenutku mjeri pojedini senzor. U samo jednom danu može se skupiti dosta podataka. A što kada to ne bi radili za jedan uređaj nego za sve izdane uređaje nekog modela? Količina podataka bi jako brzo narasla.

Kako količina podataka postaje sve veća, dolazimo do pojma \emph{Velika količina podataka} \engl{Big Data}. U današnje vrijeme imamo više podataka u digitalnom obliku nego što smo ikada imali. Jedan od zanimljivijih izazova je kako ih efektivno obraditi i zaključiti nešto iz toga. Kako od te velike količine podataka doći do nekih pametnih zaključaka iz kojih ćemo nešto novo naučiti.

\emph{Apache Spark} je otvorena \engl{open source} tehnologija koja omogućava pisanje programa za obradu podataka u tri programskih jezika: \emph{Java}, \emph{Python} i  \emph{Scala}; a nudi i mogućnost interaktivnog rada. \\
U okviru ovog rada biti će proučene mogućnosti ove tehnologije, razrađeno nekoliko konkretnih primjera obrade podataka te ostvarena programska rješenja koja obavljaju tu obradu koristeći \emph{Apache Spark}.

Svi primjeri će biti napisani u programskom jeziku \emph{Java}.

\section{Instalacija i izvorni kodovi}
\subsection{Instalacija}
\emph{Apache Spark} je pisan u programskom jeziku \emph{Scala} i izvršava se na \emph{Javinom virtualnom stroju} \engl{Java Virtual Machine} (JVM). Instalacija na osobno računalo je prilično jednostavna, a u ovdje će biti prikazana instalacija na operacijskom sustavu \emph{Ubuntu 15.10}. \\
Za početak je potrebno imati instaliranu Javu, a je li Java instalirana na računalu se može provjeriti tako što se u naredbenom retku unese sljedeća naredba:\\
\texttt{java -version}. Kao rezultat bi trebali dobiti trenutno instaliranu verziju Jave. Ukoliko Java nije instalirana, potrebno ju je najprije instalirati. Taj dio neće biti objašnjen ovdje.

Jednom kada imamo instaliranu Javu, sve što treba napraviti je otići na službene stranice: \url{https://spark.apache.org/downloads.html}, odabrati najnoviju verziju (Za vrijeme pisanja ovog rada to je verzija \emph{1.6.1 (Mar 09 2016}), izabrati odgovarajući paket te pokrenuti dohvaćanje odgovarajuće \emph{.tgz} arhive. Najjednostavnije je odabrati neki \emph{pre-built} paket, primjerice \emph{Pre-built for Hadoop 2.6 and later} te će daljnji koraci instalacije biti napisani pod pretpostavkom da je korisnik dohvatio tu verziiju paketa. Ukoliko korisnik želi, moguće je instalirati i \emph{Source code} varijantu paketa, ali taj postupak instalacije ovdje nije opisan. 

Nakon što smo dohvatili odgovarajuću arhivu, potrebno ju je raspakirati.\\ Raspakiranje arhive moguće je napraviti preko naredbe:
\begin{lstlisting}[language=bash]
$ tar -xvf spark-1.6.1-bin-hadoop2.6.tgz
\end{lstlisting}
Nakon toga, dobra je praksa premjestiti instalaciju u neki prikladniji direktorij. Tako nešto može se napraviti na sljedeći način:
\begin{lstlisting}[language=bash]
$ mv Downloads/spark-1.6.1-bin-hadoop2.6 faks/spark/
\end{lstlisting}
Službeno, sada je instalacija gotova. Idemo u sljedećem odlomku malo detaljnije pogledati što smo to zapravo dobili instalacijom. Koje datoteke se sada nalaze na našem računalu, a nije ih bilo ranije. Također, biti će objašnjena i uloga nekih datoteka i direktorija.

\subsection{Izvorni kodovi}
Ako izlistamo što nam se trenutno nalazi u novonastalom direktoriju, dobiti ćemo sljedeći ispis:
\begin{lstlisting}[language=bash]
mmatak@martins-beast:~/faks/spark$ ls -l
total 1408
drwxr-xr-x 2 mmatak mmatak    4096 Feb 27 06:02 bin
-rw-r--r-- 1 mmatak mmatak 1343562 Feb 27 06:02 CHANGES.txt
drwxr-xr-x 2 mmatak mmatak    4096 Feb 27 06:02 conf
drwxr-xr-x 3 mmatak mmatak    4096 Feb 27 06:02 data
drwxr-xr-x 3 mmatak mmatak    4096 Feb 27 06:02 ec2
drwxr-xr-x 3 mmatak mmatak    4096 Feb 27 06:02 examples
drwxr-xr-x 2 mmatak mmatak    4096 Feb 27 06:02 lib
-rw-r--r-- 1 mmatak mmatak   17352 Feb 27 06:02 LICENSE
drwxr-xr-x 2 mmatak mmatak    4096 Feb 27 06:02 licenses
-rw-r--r-- 1 mmatak mmatak   23529 Feb 27 06:02 NOTICE
drwxr-xr-x 6 mmatak mmatak    4096 Feb 27 06:02 python
drwxr-xr-x 3 mmatak mmatak    4096 Feb 27 06:02 R
-rw-r--r-- 1 mmatak mmatak    3359 Feb 27 06:02 README.md
-rw-r--r-- 1 mmatak mmatak     120 Feb 27 06:02 RELEASE
drwxr-xr-x 2 mmatak mmatak    4096 Feb 27 06:02 sbin
\end{lstlisting}

\begin{description}
\item[\emph{README.md} ]Sadrži kratke instrukcije za upoznavanje sa Spark-om.
\item[\emph{bin} ]Sadrži izvršive datoteke koje se koriste za interaktivni rad sa Spark-om.\\Zanimljivo je spomenuti da postoji interaktivna ljuska \emph{Spark shell} isključivo za programske jezike \emph{Python} i \emph{Scala}. Datoteke u ovom direktoriju služe upravo za to. Budući da je ovaj rad ograničen isključivo na programski jezik \emph{Java}, a u ovom trenutku takva interaktivna ljuska još ne postoji, ovaj dio neće biti detaljnije obrađen.
\item[\emph{core, streaming, python, ...} ]Sadrži glavne komponente projekta \emph{Apache Spark} 
\item[\emph{examples} ]Sastoji se od nekoliko jednostavnih primjera koje pomažu korisniku da se uhoda i što bezbolnije nauči koristiti odgovarajući API.
\end{description}

Kako bi bili sigurni da je instalacija uspješna, potrebno je pozicionirati se u \emph{bin} direktorij te u terminalu upisati \texttt{./spark-shell}. Ispis bi trebao biti sličan ovome:\\
\begin{lstlisting}[language=bash]
mmatak@martins-beast:~/faks/spark/bin$ ./spark-shell
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1
      /_/

Using Scala version 2.10.5 (OpenJDK 64-Bit Server VM, Java 1.8.0_91)
Type in expressions to have them evaluated.
Type :help for more information.
scala>
\end{lstlisting}


Ukoliko se prikaže greška vezana uz \emph{sqlContext}, to je OK. U ovom trenutku to nije važno. Važno je napomenuti da su u gornjem ispisu (radi ljepšeg formata ovog rada) izbrisana upozorenja \engl{warnings} koja su normalna i očekivana.

\newpage
\section{Osnovni gradivni elementi}
Osnovna programska apstrakcija s kojom \emph{Apache Spark} radi naziva se \emph{resilient distributed dataset} (RDD). RDD-ovi predstavljaju kolekcije podataka koje se nalaze na računalima (jednom ili više njih) i moguće ih je paralelno obrađivati. \emph{Apache Spark} nudi bogati API za rad s RDD-ovima.

\emph{Apache Spark} se sastoji od nekoliko ključnih elemenata koji će ovdje biti samo nabrojani i opisani rečenicom ili dvije, a u kasnijim poglavljima će biti detaljnije objašnjeni. Slika koja predstavlja osnovne elemente je sljedeća: 
%preuzeto s http://spark.apache.org/images/spark-stack.png
\begin{figure}[htb]
\centering
\includegraphics[width=10cm]{img/spark-stack.png}
\caption{Osnovni elementi tehnologije \emph{Apache Spark}.}
\label{fig:spark-stack}
\end{figure}

Na slici su prikazana četiri osnovna elementa koja čine osnovu tehnologije \emph{Apache Spark}, a to su: 
\begin{description}
\item[\emph{Spark SQL}] - omogućuje rad s bilo kakvim strukturiranim podatcima, primjerice JSON. Nudi mogućnost izvršavanja SQL-a nad RDD-ovima;
\item[\emph{Spark Streaming}] - komponenta zadužena za rad s tokovima podataka;
\item[\emph{MLib}] - koristi se za postupak strojnog učenja i 
\item[\emph{GraphX}] - biblioteka za obradu grafova (npr. graf prijatelja na društvenoj mreži).
\end{description}

\section{Kratki pregled}
U ovom poglavlju je bila ideja da čitatelj dobije motivaciju i želju za upoznavanjem s tehnologijom \emph{Apache Spark}. Detaljno je opisan postupak instalacije koji bi čitatelju trebao biti sasvim dovoljan za samostalnu instalaciju. Također, dan je pregled nekih najosnovnijih datoteka i direktorija koje dolaze s instalacijom. Zainteresiranog čitatelja se ohrabruje da samostalno prouči kako se koristi \emph{Spark shell}. Na kraju, nabrojani su osnovni elementi koji čine jezgru tehnologije \emph{Apache Spark} i uveden je pojam osnovne programske apstrakcije koja se koristi u ovoj tehnologiji, a to je \emph{resilient distributed dataset} (RDD).

\chapter{Prvi programi}
\section{Postavljanje temelja}
\subsection{Osnovni elementi aplikacije}
Općenito govoreći, svaka Spark aplikacija sastoji se od nekoliko komponenata. Prva komponenta koju ćemo spomenuti je program koji se izvršava - onaj čija je \texttt{main} metoda pokrenuta, odnosno onaj koji pokreće obradu podataka. Taj program se naziva \emph{driver}. On s podatcima priča kroz "tunel" između sebe i Sparka odnosno kroz \emph{SparkContext}. Budući da je \emph{Apache Spark} namijenjen za paralelnu obradu podataka, idemo se odmah upoznati i s još dvije komponente. Pojedino računalo u \emph{grozdu} \engl{cluster} ćemo nazvati \emph{radnim čvorom} \engl{worker node}, a proces koji se izvršava na pojedinom računalu nazvati ćemo \emph{radnik} \engl{executor}. Dozvoljeno je da \emph{radnici} međusobno komuniciraju. U cijeloj priči može, a i ne mora eksplicitno biti uključen \emph{upravitelj grozdom} \engl{cluster manager}.

%Izvor slike: http://spark.apache.org/docs/latest/cluster-overview.html

\begin{figure}[htb]
\centering
\includegraphics[width=10cm]{img/cluster-overview.png}
\caption{Prikaz elemenata aplikacije.}
\label{fig:cluster-overview}
\end{figure}
\subsection{Programiranje u Javi}
Kako bi mogli pisati Spark programe u programskom jeziku \emph{Java}, potrebno je povezati svoju aplikaciju s \texttt{spark-core} artifaktom s Mavena. Za postavljanje i instalaciju Mavena konzultirati \url{https://maven.apache.org/install.html} i knjigu (Programiranje u Javi, Čupić). Za vrijeme pisanja ovog rada, najnovija verzija Sparka je 1.6.1, a odgovarajuće Maven koordinate su:
\begin{lstlisting}[language=bash]
groupId = org.apache.spark
artifactId = spark-core_2.10
version = 1.6.1
\end{lstlisting}

Odgovarajuća \texttt{pom.xml} datoteka nalazi se u dodatku \ref{ch:datotekapomXML}.
Jednom kada smo namjestili \texttt{pom.xml} datoteku i povezali se s \texttt{spark-core}, sve što trebamo još napraviti je inicijalizirati \emph{SparkContext} i napisati prvu aplikaciju. U nastavku slijedi jednostavna aplikacija koja jedino što radi je broji koliko puta se pojedina riječ pojavljuje u tekstualnoj datoteci.
\begin{lstlisting}[numbers=left]
/**
 *
 */
package hr.fer.zemris;

import java.util.Arrays;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;

import scala.Tuple2;

/**
 * Razred ima svrhu prikazati osnovnu funkcionalnost Spark tehnologije na
 * primjeru prebrojavanja riječi u tekstualnoj datoteci. Kao rezultat program će
 * zapisati u tekstualnu datoteku koja riječ se koliko puta ponavlja. Očekuje se
 * dva argumenta kroz naredbeni redak, a to su putanja do tekstualne datoteke u
 * kojoj treba izbrojati riječi i putanja do direktorija u koji će se zapisati
 * rezultat.
 *
 * @author mmatak
 *
 */
public class BrojanjeRijeci {
	/**
	 * Metoda koja se pokrene kada se pokrene program. Očekuje putanju do
	 * datoteke s riječima i putanju do direktorija gdje će zapisati rezultat
	 * izvođenja programa.
	 *
	 * @param args
	 *            Argumenti naredbenog retka.
	 */
	@SuppressWarnings("serial")
	public static void main(String[] args) {
		String ulaznaDatoteka = args[0];
		String izlazniDirektorij = args[1];

		// inicijalizacija SparkContext-a
		SparkConf conf = new SparkConf().setMaster("local")
		.setAppName("Brojanje rijeci");
		JavaSparkContext sc = new JavaSparkContext(conf);

		// učitavanje podataka
		JavaRDD<String> ulaz = sc.textFile(ulaznaDatoteka);

		// razmak se koristi da bi razdvojio dvije riječi
		JavaRDD<String> rijeci = ulaz.flatMap(new FlatMapFunction<String, String>() {
			public Iterable<String> call(String redak) {
				return Arrays.asList(redak.split(" "));
			}
		});

		// transformiraj u parove (rijec,1) i broji
		JavaPairRDD<String, Integer> brojRijeci = rijeci
		.mapToPair(new PairFunction<String, String, Integer>() {
			public Tuple2<String, Integer> call(String rijec) {
				return new Tuple2<String, Integer>(rijec, 1);
			}
		}).reduceByKey(new Function2<Integer, Integer, Integer>() {
			public Integer call(Integer x, Integer y) {
				return x + y;
			}
		});

		// spremi rezultat u izlaznu datoteku
		brojRijeci.saveAsTextFile(izlazniDirektorij);
		// zatvori "tunel" odnosno SparkContext
		sc.close();
	}
}
\end{lstlisting}
\captionof{lstlisting}{Program koji broji koliko se puta koja riječ pojavljuje u datoteci.}

\vspace{5mm}

Analizirajmo što smo napravili. Inicijalizirali smo \emph{SparkContext} tako što smo rekli da se odvija na lokalnom računalu i zadali smo ime aplikacije. Zatim smo kreirali RDD iz tekstualne datoteke koja je predana kao argument naredbenog retka. Taj RDD nam je poslužio za kreiranje novog RDD-a koji je nastao tako što smo svaki redak razdvojili po razmaku i kreirali uređeni par (\emph{riječ}, 1). U tom uređenom paru nam \emph{riječ} predstavlja ključ, a 1 predstavlja vrijednost. Odnosno, uređeni par je oblika (\emph{ključ}, \emph{vrijednost}). Zatim smo iz tako uređenih parova, one koji imaju jednaki ključ zbrojili po vrijednostima i u tom trenutku\footnote{Nije zapravo u tom trenutku se ništa dogodilo nego tek nakon poziva metode \texttt{saveAsTextFile()}, ali o \emph{lijenoj evaluaciji} \engl{lazy evaluation} će biti govora tek kasnije.} nije više postojalo dva uređena para koji imaju jednake ključeve. Na kraju smo rezultat zapisali i eksplicitno zatvorili \emph{SparkContext}. Budući da se \emph{SparkContext} implicitno zatvori na kraju metode \texttt{main}, više ga nećemo eksplicitno zatvarati osim kada to bude potrebno.

Kako je ovo bio početni primjer, \emph{lambda} izrazi koji su uobičajeni za programski jezik \emph{Java 8} nisu korišteni iz razloga da bi čitatelj mogao lakše shvatiti što se sve treba implementirati. Odgovarajući kod koristeći lambda izraze bi izgledao ovako:
\vspace{5mm}
\begin{lstlisting}
// razmak se koristi da bi razdvojio dvije riječi
JavaRDD<String> rijeci = ulaz.flatMap(redak -> Arrays.asList(" "));

// transformiraj u parove (rijec,1) i broji
		JavaPairRDD<String, Integer> brojRijeci = rijeci
				.mapToPair(rijec -> new Tuple2<String, Integer>(rijec, 1))
				.reduceByKey((x, y) -> x + y);
\end{lstlisting}
\vspace{5mm}
\captionof{lstlisting}{Brojanje riječi koristeći lambda izraze.}

\section{Programiranje s RDD-ovima}
\emph{Resilient distributed dataset} (RDD) je osnovna podatkovna struktura Spark-a. To je \emph{nepromjenjiva} \engl{immutable} kolekcija podataka. To znači da se iz jednog RDD-a može jedino napraviti drugi RDD, a ne može se promijeniti postojeći RDD. U prethodnom primjeru to vidimo pri pozivu metode \texttt{flatMap}. Ta metoda transformira postojeći RDD \texttt{ulaz} u novi RDD \texttt{rijeci}. Sličnu transformaciju radi i metoda \texttt{mapToPair}. Drugim riječima, \emph{transformacija} \engl{transformation} je svaka metoda koja iz jednog RDD-a kreira drugi RDD. Uz transformacije, postoje i \emph{akcije} \engl{actions}. Akcije se razlikuju od transformacija po tome što ne vraćaju novi RDD nego vraćaju jedan ili više elemenata iz nekog RDD-a, ili kao u našem primjeru s brojanjem riječi, zapisuju RDD u obliku tekstualne datoteke - metoda \texttt{saveAsTextFile}.

U tablici \ref{tbl:transformacije} mogu se naći neke od mogućih transformacija i njihovi opisi, a u tablici \ref{tbl:akcije} nalaze se akcije koje je moguće pozvati zajedno s odgovarajućim opisima.
\begin{table}[htb]
\caption{Neke od transformacija}
\label{tbl:transformacije}
\centering
\begin{tabular}{lp{10cm}} 
\hline
Transformacija & Opis \\
\hline
\texttt{map(\emph{func})} & Vraća novi RDD nastao tako što je svaki element orginalnog RDD-a predan funckiji \emph{func}. \\
\texttt{filter(\emph{func})} & Vraća novi RDD nastao tako što su iz orginalnog RDD-a preuzeti samo oni elementi za koje funkcija \emph{func} vraća \texttt{true}. \\
\texttt{flatMap(\emph{func})} & Slično kao \texttt{map(\emph{func})}, ali jedan ulazni element kreira 0 ili više izlaznih elemenata. \\
\texttt{union(\emph{drugiRDD})} & Vraća uniju između RDD-a nad kojim je akcija pozana i RDD-a koji je predan kao argument. \\
\texttt{intersection(\emph{drugiRDD})} & Vraća presjek između RDD-a nad kojim je akcija pozana i RDD-a koji je predan kao argument. \\
\hline
\end{tabular}
\end{table}

Za opis ostalih transformacija, pogledati \url{http://spark.apache.org/docs/latest/programming-guide.html#transformations}.
\newpage
\begin{table}[htb]
\scriptsize
\caption{Akcije}
\label{tbl:akcije}
\centering
\begin{tabular}{lp{10cm}} 
\hline
Akcija & Opis \\
\hline
\texttt{reduce(\emph{func})} & Agregacija elemenata iz RDD-a tako što se nad dva elementa pozove funkcije \emph{func}, a ta funkcija vrati jedan element. Funkcija \emph{func} bi trebala biti komutativna i asocijativna kako bi se pravilno izvršavala u paralelnoj obradi podataka.\\
\texttt{collect()} & Vraća polje svih elemenata iz RDD-a direktno u \emph{driver} program. Budući da je tih elemenata puno, u praksi se poziva nakon transformacije \texttt{filter()}. \\
\texttt{count()} & Vraća broj elemenata u RDD-u \\
\texttt{take(\emph{n})} & Vraća polje prvih \emph{n} elemenata iz RDD-a. \\
\texttt{first()} & Vraća prvi element iz RDD-a. Može se ostvariti i pozivom metode  \texttt{take(\emph{1})}\\
\hline
\end{tabular}
\end{table}
Za opis ostalih akcija, pogledati \url{http://spark.apache.org/docs/latest/programming-guide.html#actions}.
\\
\\
Budući da se radi o velikoj količini podataka, evaluacija transformacija je \emph{lijena} \engl{lazy evaluation}. To znači da Spark samo pamti koje sve transformacije treba napraviti nad RDD-ovima, ali ne i da ih odmah odradi. Sve potrebne transformacije budu napravljene tek pozivom prve akcije. 

U nastavku slijedi primjer koji iz datoteke \texttt{logfile.txt} broji koliko puta je zahtjev bio na URL koji u sebi sadrži riječ "burza" i koliko je puta došao zahtjev na URL koji u sebi sadrži riječ "index". Njihova unija je također izračunata.
 
\vspace{5mm}
\begin{lstlisting}
package hr.fer.zemris;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class Primjer3 {
	public static void main(String[] args) {
		// inicijalizacija SparkContext-a
		SparkConf conf = new SparkConf().setMaster("local")
				.setAppName("Brojanje rijeci");
		JavaSparkContext sc = new JavaSparkContext(conf);

		// učitavanje podataka
		JavaRDD<String> ulaz = sc.textFile("logfile.txt");

		// transformacije
		JavaRDD<String> burze = ulaz.filter(redak -> redak.contains("burza"));
		JavaRDD<String> indexi = ulaz.filter(redak -> redak.contains("index"));
		JavaRDD<String> burzeUnijaIndexi = burze.union(indexi);

		// akcije
		long brojLinija = burzeUnijaIndexi.count();
		long ukupanBrojLinija = ulaz.count();
		System.out.println(
				"Broj linija koje sadrže riječ 'burza' ili 'index' je: "
						+ brojLinija + ", odnosno "
						+ (double) 100 * brojLinija / ukupanBrojLinija + "%.");
		System.out.println(
				"Prva linija koja sadrži riječ 'burza' ili 'index' je: \n"
						+ burzeUnijaIndexi.first());
	}
}
\end{lstlisting}
\captionof{lstlisting}{Korištenje transformacija i akcija.}
\vspace{5mm}

Ovdje imamo 4 RDD-a: \texttt{ulaz}, \texttt{burze}, \texttt{indexi} i \texttt{burzeUnijaIndexi}. RDD \texttt{burze} i RDD \texttt{indexi} su kreirani na temelju RDD-a \texttt{ulaz}. RDD \texttt{burzeUnijaIndexi} je kreiran na temelju RDD-a \texttt{burze} i na temelju RDD-a \texttt{indexi}. U nastavku je graf koji to opisuje.

\begin{figure}[htb]
\centering
\includegraphics[width=10cm]{img/burzeUnijaIndexiRDD.png}
\caption{Transformacije nad RDD-ovima.}
\label{fig:burzeUnijaIndexiRDD}
\end{figure}

Tek prilikom poziva akcije \texttt{burzaUnijaIndexi.count()} se zapravo kreira RDD \texttt{ulaz}, na temelju njega RDD \texttt{burza} i RDD \texttt{indexi} i onda tek na temelju njih se kreira RDD \texttt{burzaUnijaIndexi} te se tek onda računa ukupni broj linija u tom RDD-u. Nakon što se to izračuna, svi RDD-ovi nestaju iz memorije.  Prilikom poziva akcije \texttt{burzeUnijaIndexi.first()} \textbf{ponovno} se kreiraju prethodno navedeni RDD-i i sve ide iz početka.

Na prvi pogled ovo izgleda kao loša implementacija, ali budući da se ovdje radi o velikoj količini podataka i ne možemo ih nikako sve pohraniti u memoriju, ovo je zapravo logična implementacija. Ukoliko ne želimo svaki puta iz početka računati i kreirati RDD \texttt{burzaUnijaIndexi}, trebamo ga \texttt{perzistirati} pozivom metode \texttt{persist()} i predavanjem odgovarajućeg parametra (pogledati tablicu \ref{tbl:razinePerzistencije}), odnosno spremiti ga u memoriju ili na disk. Ako ga imamo spremljenog, ne treba ga ponovno računati. Razine perzistencije dane su u tablici u \ref{tbl:razinePerzistencije}.  

\begin{table}[htb]
\scriptsize
\caption{Razine perzistencije}
\label{tbl:razinePerzistencije}
\centering
\begin{tabular}{lllll} 
\hline
Razina & Prostorno zauzeće & Procesorsko vrijeme & U memoriji & Na disku\\
\hline
\texttt{MEMORY\_ONLY} & Visoko & Nisko & Da & Ne  \\
\texttt{MEMORY\_ONLY\_SER} & Nisko & Visoko & Da & Ne  \\
\texttt{MEMORY\_AND\_DISK} & Visoko & Srednje & Dio & Dio \\
\texttt{MEMORY\_AND\_DISK\_SER} & Nisko & Visoko & Dio & Dio \\
\texttt{DISK\_ONLY} & Nisko & Visoko & Ne & Da \\
\hline
\end{tabular}
\end{table}

\section{Kratki pregled}
U ovom poglavlju smo postavili temelje nad kojima ćemo kasnije graditi složenije aplikacije. Vrlo je važno da čitatelj jasno razumije što je to RDD jer bez toga nema smisla dalje pratiti ovaj rad. Dan je pregled osnovnih transformacija i akcija koje zapravo, uz RDD, čine jezgru tehnologije \emph{Apache Spark}. Također je uveden i pojam perzistencije za efektivniji rad s RDD-ovima.
\chapter{Zaključak} 
Zaključak.

\begin{thebibliography}{9}
\bibitem{officialDocumentation}
  Službena dokumentacija projekta \emph{Apache Spark} \url{http://spark.apache.org/docs/latest/}
\bibitem{learningSpark}
  Holden Karau, Andy Konwinski, Patrick Wendell i Matei Zaharia
  \emph{Learning Spark, lighting-fast}
\bibitem{marcupic}
  Marko Čupić,
  \emph{Programiranje u Javi},
  verzija 0.3.26
\bibitem{latexPredlozak}
  Ivan Krišto, Boran Car, Mateja Čuljak, Vedrana Janković, Hrvoje Bandov
  \emph{Upute za korištenje \LaTeX predloška za Završni i Diplomski rad te seminar},
  godina 2010.
\end{thebibliography}

\newpage
% Dodatak nije obavezan
\begin{appendices}
\chapter{Postavljanje datoteke pom.xml}
\label{ch:datotekapomXML}
Datoteka koja je potrebna kako bi Maven ispravno dohvatio artifakt \texttt{spark-core} bi trebala izgledati slično kao što je dano u nastavku. Za ispravno funkcioniranje, trebala bi se zvati \texttt{pom.xml}.
\begin{lstlisting}[language=XML]
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>hr.fer.zemris</groupId>
  <artifactId>Prvi-programi</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <name>Prvi programi</name>
  <description>Prvi programi u Spark-u</description>
  <dependencies>
  	<dependency>
  		<groupId>org.apache.spark</groupId>
  		<artifactId>spark-core_2.10</artifactId>
  		<version>1.6.1</version>
  	</dependency>
  </dependencies>
</project>
\end{lstlisting}
\end{appendices}

\begin{sazetak}
Sažetak na hrvatskom jeziku.

\kljucnerijeci{Ključne riječi, odvojene zarezima.}
\end{sazetak}

% TODO: Navedite naslov na engleskom jeziku.
\engtitle{Title}
\begin{abstract}
Abstract.

\keywords{Keywords.}
\end{abstract}

\end{document}
